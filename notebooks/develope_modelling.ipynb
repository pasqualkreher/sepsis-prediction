{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GroupKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import warnings \n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from py_funcs.func_ml import *\n",
    "from py_funcs.func_project_dir import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_functions.make_train_test_ids(onset_hour=5, test_size=0.2)\n",
    "df_train_ids    = pd.read_csv(project_path()+r\"/machine_learning/df_train_ids.csv\", index_col=0)\n",
    "df_test_ids     = pd.read_csv(project_path()+r\"/machine_learning/df_train_ids.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest and XGBoost - preselected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipline use the preselected features from the jupyter notebook\n",
    "'feature_selection.ipynb' in folder 'notebooks' you can experiment\n",
    "in the notebook, the inital one will be the one of this research "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.9s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'log2', 'model__min_samples_leaf': 23, 'model__min_samples_split': 9, 'model__n_estimators': 347}\n",
      "Train Accuracy for RandomForest: 0.7364\n",
      "Test Accuracy for RandomForest: 0.6682\n",
      "Train Precision for RandomForest: 0.8026\n",
      "Test Precision for RandomForest: 0.6982\n",
      "Train Recall for RandomForest: 0.6269\n",
      "Test Recall for RandomForest: 0.5925\n",
      "Computation Time for RandomForest: 919.43 seconds\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   1.3s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.04854165025399162), 'model__max_depth': 2, 'model__n_estimators': 574, 'model__subsample': np.float64(0.7078044430599341)}\n",
      "Train Accuracy for XGBClassifier: 0.7288\n",
      "Test Accuracy for XGBClassifier: 0.6786\n",
      "Train Precision for XGBClassifier: 0.7405\n",
      "Test Precision for XGBClassifier: 0.6803\n",
      "Train Recall for XGBClassifier: 0.7044\n",
      "Test Recall for XGBClassifier: 0.6739\n",
      "Computation Time for XGBClassifier: 121.04 seconds\n"
     ]
    }
   ],
   "source": [
    "df_rf_feature_selection = pd.read_csv(project_path()+r\"/data/target_data/rf_feature_selection.csv\", index_col=0)\n",
    "df_ml = pd.read_csv(project_path()+r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_ml = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=12, hours_before_sepsis_cutoff=0)\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\") \n",
    "df_ml_test = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\")\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test = df_ml_test.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "pipelines = {\n",
    "    'RandomForest': Pipeline([\n",
    "        ('model', RandomForestClassifier(random_state=42))\n",
    "    ], verbose=True),\n",
    "\n",
    "    'XGBClassifier': Pipeline([\n",
    "        ('model', XGBClassifier(random_state=42))\n",
    "    ], verbose=True),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': randint(10, 600),\n",
    "        'model__max_depth': randint(1, 9),\n",
    "        'model__min_samples_split': randint(2, 25),\n",
    "        'model__min_samples_leaf': randint(1, 25),\n",
    "        'model__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model__n_estimators': randint(10, 600),\n",
    "        'model__max_depth': randint(1, 9),\n",
    "        'model__learning_rate': uniform(0.01, 0.1),\n",
    "        'model__subsample': uniform(0.5, 0.9),\n",
    "    },\n",
    "}\n",
    "\n",
    "gkf = GroupKFold(n_splits=20)\n",
    "groups = df_ml_train[\"ICUSTAY_ID\"]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Perform RandomizedSearchCV for each pipeline\n",
    "for name, pipeline in pipelines.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    search = RandomizedSearchCV(pipeline, param_distributions=param_grids[name], \n",
    "                                n_iter=20, cv=gkf, random_state=42, n_jobs=-1, verbose=3)\n",
    "    search.fit(X_train, y_train, groups=groups)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    best_model = search.best_estimator_\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    \n",
    "    model_filename = project_path()+fr\"/machine_learning/models/{name}_presel.joblib\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    \n",
    "    # Store feature importances directly in the results dictionary\n",
    "    feature_importances = None\n",
    "    if hasattr(best_model.named_steps['model'], 'feature_importances_'):\n",
    "        feature_importances = pd.Series(\n",
    "            best_model.named_steps['model'].feature_importances_,\n",
    "            index=X_train.columns\n",
    "        ).to_dict() \n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[name] = {\n",
    "        'model': name,\n",
    "        'pipeline': 'preselected features',\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_precision': train_precision,\n",
    "        'test_precision': test_precision,\n",
    "        'train_recall': train_recall,\n",
    "        'test_recall': test_recall,\n",
    "        'best_params': search.best_params_,\n",
    "        'computation_time': end_time - start_time,\n",
    "        'model_path': model_filename,\n",
    "        'feature_importances': feature_importances,\n",
    "        'selected_features': X_train.columns.tolist()\n",
    "    }\n",
    "\n",
    "    print(f\"Best model for {name}: {search.best_params_}\")\n",
    "    print(f\"Train Accuracy for {name}: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy for {name}: {test_accuracy:.4f}\")\n",
    "    print(f\"Train Precision for {name}: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision for {name}: {test_precision:.4f}\")\n",
    "    print(f\"Train Recall for {name}: {train_recall:.4f}\")\n",
    "    print(f\"Test Recall for {name}: {test_recall:.4f}\")\n",
    "    print(f\"Computation Time for {name}: {results[name]['computation_time']:.2f} seconds\")\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path()+fr\"/machine_learning/results/ml_results_presel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest and XGBoost - all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  33.1s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'log2', 'model__min_samples_leaf': 3, 'model__min_samples_split': 23, 'model__n_estimators': 318}\n",
      "Train Accuracy for RandomForest: 0.7480\n",
      "Test Accuracy for RandomForest: 0.6677\n",
      "Train Precision for RandomForest: 0.7985\n",
      "Test Precision for RandomForest: 0.6897\n",
      "Train Recall for RandomForest: 0.6634\n",
      "Test Recall for RandomForest: 0.6097\n",
      "Computation Time for RandomForest: 1286.58 seconds\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   5.7s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.04854165025399162), 'model__max_depth': 2, 'model__n_estimators': 574, 'model__subsample': np.float64(0.7078044430599341)}\n",
      "Train Accuracy for XGBClassifier: 0.7353\n",
      "Test Accuracy for XGBClassifier: 0.6877\n",
      "Train Precision for XGBClassifier: 0.7478\n",
      "Test Precision for XGBClassifier: 0.6875\n",
      "Train Recall for XGBClassifier: 0.7100\n",
      "Test Recall for XGBClassifier: 0.6882\n",
      "Computation Time for XGBClassifier: 604.36 seconds\n"
     ]
    }
   ],
   "source": [
    "df_ml = pd.read_csv(project_path() + r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_ml = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=12, hours_before_sepsis_cutoff=0)\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\")\n",
    "df_ml_test = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\")\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test = df_ml_test.iloc[:, 10:]\n",
    "y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "pipelines = {\n",
    "    'RandomForest': Pipeline([\n",
    "        ('model', RandomForestClassifier(random_state=42))\n",
    "    ], verbose=True),\n",
    "\n",
    "    'XGBClassifier': Pipeline([\n",
    "        ('model', XGBClassifier(random_state=42))\n",
    "    ], verbose=True),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': randint(10, 600),\n",
    "        'model__max_depth': randint(1, 9),\n",
    "        'model__min_samples_split': randint(2, 25),\n",
    "        'model__min_samples_leaf': randint(1, 25),\n",
    "        'model__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model__n_estimators': randint(10, 600),\n",
    "        'model__max_depth': randint(1, 9),\n",
    "        'model__learning_rate': uniform(0.01, 0.1),\n",
    "        'model__subsample': uniform(0.5, 0.9),\n",
    "    },\n",
    "}\n",
    "\n",
    "gkf = GroupKFold(n_splits=20)\n",
    "groups = df_ml_train[\"ICUSTAY_ID\"]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Perform RandomizedSearchCV for each pipeline\n",
    "for name, pipeline in pipelines.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    search = RandomizedSearchCV(pipeline, param_distributions=param_grids[name], \n",
    "                                n_iter=20, cv=gkf, random_state=42, n_jobs=-1, verbose=3)\n",
    "    search.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "    # Save the best model\n",
    "    model_filename = project_path() + fr\"/machine_learning/models/{name}_all.joblib\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "\n",
    "    # Extract feature importances if the model supports it\n",
    "    feature_importances = None\n",
    "    if hasattr(best_model.named_steps['model'], 'feature_importances_'):\n",
    "        feature_importances = pd.Series(\n",
    "            best_model.named_steps['model'].feature_importances_,\n",
    "            index=X_train.columns\n",
    "        ).to_dict()\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[name] = {\n",
    "        'model': name,\n",
    "        'pipeline': 'all features',\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_precision': train_precision,\n",
    "        'test_precision': test_precision,\n",
    "        'train_recall': train_recall,\n",
    "        'test_recall': test_recall,\n",
    "        'best_params': search.best_params_,\n",
    "        'computation_time': end_time - start_time,\n",
    "        'model_path': model_filename,\n",
    "        'feature_importances': feature_importances,\n",
    "        'selected_features': X_train.columns.tolist()\n",
    "    }\n",
    "\n",
    "    print(f\"Best model for {name}: {search.best_params_}\")\n",
    "    print(f\"Train Accuracy for {name}: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy for {name}: {test_accuracy:.4f}\")\n",
    "    print(f\"Train Precision for {name}: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision for {name}: {test_precision:.4f}\")\n",
    "    print(f\"Train Recall for {name}: {train_recall:.4f}\")\n",
    "    print(f\"Test Recall for {name}: {test_recall:.4f}\")\n",
    "    print(f\"Computation Time for {name}: {results[name]['computation_time']:.2f} seconds\")\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path() + fr\"/machine_learning/results/ml_results_all.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest, XGBoost, Logistic Regression and SVM - Standard Scaled + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'log2', 'model__min_samples_leaf': 24, 'model__min_samples_split': 9, 'model__n_estimators': 347}\n",
      "Train Accuracy for RandomForest: 0.7344\n",
      "Test Accuracy for RandomForest: 0.6521\n",
      "Train Precision for RandomForest: 0.7670\n",
      "Test Precision for RandomForest: 0.6654\n",
      "Train Recall for RandomForest: 0.6733\n",
      "Test Recall for RandomForest: 0.6117\n",
      "Computation Time for RandomForest: 3441.14 seconds\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.08851759613930137), 'model__max_depth': 3, 'model__n_estimators': 373, 'model__subsample': np.float64(0.9628109945722505)}\n",
      "Train Accuracy for XGBClassifier: 0.7385\n",
      "Test Accuracy for XGBClassifier: 0.6598\n",
      "Train Precision for XGBClassifier: 0.7508\n",
      "Test Precision for XGBClassifier: 0.6609\n",
      "Train Recall for XGBClassifier: 0.7141\n",
      "Test Recall for XGBClassifier: 0.6565\n",
      "Computation Time for XGBClassifier: 423.81 seconds\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Best model for LogisticRegression: {'model__C': np.float64(83.25426408004218), 'model__penalty': 'l2', 'model__solver': 'saga'}\n",
      "Train Accuracy for LogisticRegression: 0.6472\n",
      "Test Accuracy for LogisticRegression: 0.6460\n",
      "Train Precision for LogisticRegression: 0.6525\n",
      "Test Precision for LogisticRegression: 0.6434\n",
      "Train Recall for LogisticRegression: 0.6300\n",
      "Test Recall for LogisticRegression: 0.6549\n",
      "Computation Time for LogisticRegression: 337.91 seconds\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Best model for SVM: {'model__C': np.float64(0.21584494295802448), 'model__degree': 3, 'model__kernel': 'rbf'}\n",
      "Train Accuracy for SVM: 0.5454\n",
      "Test Accuracy for SVM: 0.5393\n",
      "Train Precision for SVM: 0.5500\n",
      "Test Precision for SVM: 0.5433\n",
      "Train Recall for SVM: 0.4992\n",
      "Test Recall for SVM: 0.4929\n",
      "Computation Time for SVM: 9522.45 seconds\n"
     ]
    }
   ],
   "source": [
    "df_ml = pd.read_csv(project_path()+r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_ml = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=12, hours_before_sepsis_cutoff=0)\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"]) \n",
    "df_ml_test  = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"])\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test  = df_ml_test.iloc[:, 10:]\n",
    "y_test  = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "\n",
    "pipelines = {\n",
    "    'RandomForest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=0.95)),\n",
    "        ('model', RandomForestClassifier(random_state=42))\n",
    "    ]),\n",
    "\n",
    "    'XGBClassifier': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=0.95)),\n",
    "        ('model', XGBClassifier(random_state=42))\n",
    "    ]),\n",
    "\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=0.95)),\n",
    "        ('model', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'SVM': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=0.95)),\n",
    "        ('model', SVC(probability=True, random_state=42, max_iter=1000))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Define the parameter grids for each model\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': randint(10, 600),\n",
    "        'model__max_depth': randint(1, 9),\n",
    "        'model__min_samples_split': randint(2, 25),\n",
    "        'model__min_samples_leaf': randint(2, 25),\n",
    "        'model__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model__n_estimators': randint(10, 450),\n",
    "        'model__max_depth': randint(1, 9),\n",
    "        'model__learning_rate': uniform(0.01, 0.1),\n",
    "        'model__subsample': uniform(0.5, 0.9),\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model__solver': ['liblinear', 'saga'],\n",
    "        'model__penalty': ['l1', 'l2'],\n",
    "        'model__C': uniform(0.01, 100),\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model__C': uniform(0.01, 10),\n",
    "        'model__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'model__degree': randint(2, 5),\n",
    "    }\n",
    "}\n",
    "\n",
    "gkf = GroupKFold(n_splits=20)\n",
    "groups = df_ml_train[\"ICUSTAY_ID\"]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    search = RandomizedSearchCV(pipeline, param_distributions=param_grids[name], \n",
    "                                n_iter=20, cv=gkf, random_state=42, n_jobs=-1, verbose=3)\n",
    "    search.fit(X_train, y_train, groups=groups)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Save the best model\n",
    "    model_filename = project_path() + f\"/machine_learning/models/{name}_pca.joblib\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    \n",
    "    # Get number of PCA components used\n",
    "    if 'pca' in best_model.named_steps:\n",
    "        n_components_used = best_model.named_steps['pca'].n_components_\n",
    "    else:\n",
    "        n_components_used = None\n",
    "    \n",
    "    # Store the results in the dictionary\n",
    "    results[name] = {\n",
    "        'model': name,\n",
    "        'pipeline': 'pca, standard scaled',\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_precision': train_precision,\n",
    "        'test_precision': test_precision,\n",
    "        'train_recall': train_recall,\n",
    "        'test_recall': test_recall,\n",
    "        'best_params': search.best_params_,\n",
    "        'computation_time': end_time - start_time,\n",
    "        'model_path': model_filename,\n",
    "        'feature_importances': None,\n",
    "        'selected_features': f\"Number of Core Components: {n_components_used}\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Best model for {name}: {search.best_params_}\")\n",
    "    print(f\"Train Accuracy for {name}: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy for {name}: {test_accuracy:.4f}\")\n",
    "    print(f\"Train Precision for {name}: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision for {name}: {test_precision:.4f}\")\n",
    "    print(f\"Train Recall for {name}: {train_recall:.4f}\")\n",
    "    print(f\"Test Recall for {name}: {test_recall:.4f}\")\n",
    "    print(f\"Computation Time for {name}: {results[name]['computation_time']:.2f} seconds\")\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path() + f\"/machine_learning/results/ml_results_pca.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and SVM - Standard Scaled + Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipline use the preselected alpha from the jupyter notebook\n",
    "'feature_selection.ipynb' in folder 'notebooks' you can experiment\n",
    "in the notebook, the inital one will be the one of this research \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Best model for LogisticRegression: {'model__C': np.float64(60.121501174320876), 'model__penalty': 'l2', 'model__solver': 'liblinear'}\n",
      "Train Accuracy for LogisticRegression: 0.6565\n",
      "Test Accuracy for LogisticRegression: 0.6503\n",
      "Train Precision for LogisticRegression: 0.6607\n",
      "Test Precision for LogisticRegression: 0.6471\n",
      "Train Recall for LogisticRegression: 0.6434\n",
      "Test Recall for LogisticRegression: 0.6611\n",
      "Computation Time for LogisticRegression: 425.04 seconds\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Best model for SVM: {'model__C': np.float64(9.193204020787821), 'model__degree': 3, 'model__kernel': 'rbf'}\n",
      "Train Accuracy for SVM: 0.5486\n",
      "Test Accuracy for SVM: 0.5381\n",
      "Train Precision for SVM: 0.5945\n",
      "Test Precision for SVM: 0.5731\n",
      "Train Recall for SVM: 0.3055\n",
      "Test Recall for SVM: 0.2985\n",
      "Computation Time for SVM: 6205.20 seconds\n"
     ]
    }
   ],
   "source": [
    "# Lade die Daten\n",
    "df_ml = pd.read_csv(project_path() + r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_lr_feature_selection = pd.read_csv(project_path() + r\"/data/target_data/lf_feature_selection.csv\", index_col=0)\n",
    "alpha = df_lr_feature_selection[\"Alpha\"][0].item()\n",
    "df_ml = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=12, hours_before_sepsis_cutoff=0)\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"])\n",
    "df_ml_test = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"])\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test = df_ml_test.iloc[:, 10:]\n",
    "y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "pipelines = {\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lasso', SelectFromModel(Lasso(alpha=alpha, random_state=42))),\n",
    "        ('model', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'SVM': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lasso', SelectFromModel(Lasso(alpha=alpha, random_state=42))),\n",
    "        ('model', SVC(probability=True, random_state=42, max_iter=1000))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Definiere die Parametergrids für jedes Modell\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'model__solver': ['liblinear', 'saga'],\n",
    "        'model__penalty': ['l1', 'l2'],\n",
    "        'model__C': uniform(0.01, 100),\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model__C': uniform(0.1, 10),\n",
    "        'model__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'model__degree': randint(2, 5),\n",
    "    }\n",
    "}\n",
    "\n",
    "gkf = GroupKFold(n_splits=20)\n",
    "groups = df_ml_train[\"ICUSTAY_ID\"]\n",
    "\n",
    "# Dictionary zum Speichern der Ergebnisse\n",
    "results = {}\n",
    "\n",
    "# Durchführen der RandomizedSearchCV für jede Pipeline\n",
    "for name, pipeline in pipelines.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    search = RandomizedSearchCV(pipeline, param_distributions=param_grids[name], \n",
    "                                n_iter=20, cv=gkf, random_state=42, n_jobs=-1, verbose=3)\n",
    "    search.fit(X_train, y_train, groups=groups)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Bestes Modell erhalten\n",
    "    best_model = search.best_estimator_\n",
    "    \n",
    "    # Vorhersagen treffen\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Bestes Modell speichern\n",
    "    model_filename = project_path() + fr\"/machine_learning/models/{name}_lasso_alpha_fix.joblib\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    \n",
    "    # Feature-Selektionsschritt speichern\n",
    "    selected_features = X_train.columns[best_model.named_steps['lasso'].get_support()]\n",
    "    feature_importances = None\n",
    "    if hasattr(best_model.named_steps['model'], 'coef_'):\n",
    "        feature_importances = pd.Series(\n",
    "            best_model.named_steps['model'].coef_[0],\n",
    "            index=selected_features\n",
    "        )\n",
    "    \n",
    "    # Ergebnisse im Dictionary speichern\n",
    "    results[name] = {\n",
    "        'model': name,\n",
    "        'pipeline': 'standard scaled, lasso',\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_precision': train_precision,\n",
    "        'test_precision': test_precision,\n",
    "        'train_recall': train_recall,\n",
    "        'test_recall': test_recall,\n",
    "        'best_params': search.best_params_,\n",
    "        'computation_time': end_time - start_time,\n",
    "        'model_path': model_filename,\n",
    "        'feature_importances': feature_importances,\n",
    "        'selected_features': X_train.columns.tolist()\n",
    "\n",
    "    }\n",
    "    \n",
    "    print(f\"Best model for {name}: {search.best_params_}\")\n",
    "    print(f\"Train Accuracy for {name}: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy for {name}: {test_accuracy:.4f}\")\n",
    "    print(f\"Train Precision for {name}: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision for {name}: {test_precision:.4f}\")\n",
    "    print(f\"Train Recall for {name}: {train_recall:.4f}\")\n",
    "    print(f\"Test Recall for {name}: {test_recall:.4f}\")\n",
    "    print(f\"Computation Time for {name}: {results[name]['computation_time']:.2f} seconds\")\n",
    "\n",
    "# Ergebnisse als CSV speichern\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path() + fr\"/machine_learning/results/ml_results_lasso_alpha_fix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Standard Scaled + Lasso searching Alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "Best model: {'lasso__estimator__alpha': np.float64(0.0018194260557609198), 'model__C': np.float64(90.94204020787821), 'model__penalty': 'l2', 'model__solver': 'saga'}\n",
      "Train Accuracy: 0.6766\n",
      "Test Accuracy: 0.6643\n",
      "Train Precision: 0.6835\n",
      "Test Precision: 0.6631\n",
      "Train Recall: 0.6580\n",
      "Test Recall: 0.6679\n",
      "Computation Time: 363.34 seconds\n"
     ]
    }
   ],
   "source": [
    "# Lade die Daten\n",
    "df_ml = pd.read_csv(project_path() + r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_ml = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=12, hours_before_sepsis_cutoff=0)\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"])\n",
    "df_ml_test = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"])\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test = df_ml_test.iloc[:, 10:]\n",
    "y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "# Definiere die Pipeline für die Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso', SelectFromModel(Lasso(random_state=42))),  \n",
    "    ('model', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Parametergrid definieren\n",
    "param_grid = {\n",
    "    'lasso__estimator__alpha': uniform(0.0001, 0.05),\n",
    "    'model__solver': ['liblinear', 'saga'],\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__C': uniform(0.01, 100),\n",
    "}\n",
    "gkf = GroupKFold(n_splits=20)\n",
    "groups = df_ml_train[\"ICUSTAY_ID\"]\n",
    "\n",
    "# Dictionary zum Speichern der Ergebnisse\n",
    "results = {}\n",
    "\n",
    "# RandomizedSearchCV für die Pipeline durchführen\n",
    "start_time = time.time()\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_distributions=param_grid, \n",
    "                            n_iter=20, cv=gkf, random_state=42, n_jobs=-1, verbose=3)\n",
    "search.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Bestes Modell erhalten\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Metriken berechnen\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "# Bestes Modell speichern\n",
    "model_filename = project_path() + fr\"/machine_learning/models/LogisticRegression_lasso_alpha_search.joblib\"\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "# Feature-Selektionsschritt speichern\n",
    "selected_features = X_train.columns[best_model.named_steps['lasso'].get_support()]\n",
    "feature_importances = None\n",
    "if hasattr(best_model.named_steps['model'], 'coef_'):\n",
    "    # Koeffizienten als Feature-Importanzen verwenden\n",
    "    feature_importances = pd.Series(\n",
    "        best_model.named_steps['model'].coef_[0],\n",
    "        index=selected_features\n",
    "    )\n",
    "\n",
    "# Ergebnisse im Dictionary speichern\n",
    "results['LogisticRegression'] = {\n",
    "    'model': 'LogisticRegression',\n",
    "    'pipeline': 'standard scaled, lasso alpha_search',\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'train_precision': train_precision,\n",
    "    'test_precision': test_precision,\n",
    "    'train_recall': train_recall,\n",
    "    'test_recall': test_recall,\n",
    "    'best_params': search.best_params_,\n",
    "    'computation_time': end_time - start_time,\n",
    "    'model_path': model_filename,\n",
    "    'feature_importances': feature_importances,\n",
    "    'selected_features': selected_features.tolist(),\n",
    "}\n",
    "\n",
    "print(f\"Best model: {search.best_params_}\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Train Precision: {train_precision:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Train Recall: {train_recall:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Computation Time: {results['LogisticRegression']['computation_time']:.2f} seconds\")\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path() + fr\"/machine_learning/results/ml_results_lasso_alpha_search.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest and XGBoost - Fitted model for each hour from 1 to 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   1.6s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 15, 'model__n_estimators': 90}\n",
      "Train Accuracy for RandomForest: 0.7683\n",
      "Test Accuracy for RandomForest: 0.6777\n",
      "Train Precision for RandomForest: 0.8232\n",
      "Test Precision for RandomForest: 0.7023\n",
      "Train Recall for RandomForest: 0.6833\n",
      "Test Recall for RandomForest: 0.6171\n",
      "Computation Time for RandomForest: 166.92 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.4s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.08851759613930137), 'model__max_depth': 3, 'model__n_estimators': 373, 'model__subsample': np.float64(0.9628109945722505)}\n",
      "Train Accuracy for XGBClassifier: 0.8323\n",
      "Test Accuracy for XGBClassifier: 0.7076\n",
      "Train Precision for XGBClassifier: 0.8455\n",
      "Test Precision for XGBClassifier: 0.7110\n",
      "Train Recall for XGBClassifier: 0.8132\n",
      "Test Recall for XGBClassifier: 0.6995\n",
      "Computation Time for XGBClassifier: 72.13 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   4.0s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'log2', 'model__min_samples_leaf': 24, 'model__min_samples_split': 9, 'model__n_estimators': 347}\n",
      "Train Accuracy for RandomForest: 0.7216\n",
      "Test Accuracy for RandomForest: 0.6720\n",
      "Train Precision for RandomForest: 0.7721\n",
      "Test Precision for RandomForest: 0.7032\n",
      "Train Recall for RandomForest: 0.6288\n",
      "Test Recall for RandomForest: 0.5953\n",
      "Computation Time for RandomForest: 168.81 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.4s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.08851759613930137), 'model__max_depth': 3, 'model__n_estimators': 373, 'model__subsample': np.float64(0.9628109945722505)}\n",
      "Train Accuracy for XGBClassifier: 0.8321\n",
      "Test Accuracy for XGBClassifier: 0.6891\n",
      "Train Precision for XGBClassifier: 0.8417\n",
      "Test Precision for XGBClassifier: 0.6916\n",
      "Train Recall for XGBClassifier: 0.8180\n",
      "Test Recall for XGBClassifier: 0.6825\n",
      "Computation Time for XGBClassifier: 72.04 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   8.4s\n",
      "Best model for RandomForest: {'model__max_depth': 7, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 2, 'model__min_samples_split': 8, 'model__n_estimators': 530}\n",
      "Train Accuracy for RandomForest: 0.7280\n",
      "Test Accuracy for RandomForest: 0.6725\n",
      "Train Precision for RandomForest: 0.7926\n",
      "Test Precision for RandomForest: 0.7082\n",
      "Train Recall for RandomForest: 0.6177\n",
      "Test Recall for RandomForest: 0.5867\n",
      "Computation Time for RandomForest: 174.58 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.3s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.08851759613930137), 'model__max_depth': 3, 'model__n_estimators': 373, 'model__subsample': np.float64(0.9628109945722505)}\n",
      "Train Accuracy for XGBClassifier: 0.8304\n",
      "Test Accuracy for XGBClassifier: 0.6796\n",
      "Train Precision for XGBClassifier: 0.8440\n",
      "Test Precision for XGBClassifier: 0.6769\n",
      "Train Recall for XGBClassifier: 0.8106\n",
      "Test Recall for XGBClassifier: 0.6872\n",
      "Computation Time for XGBClassifier: 70.85 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   1.6s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 15, 'model__n_estimators': 90}\n",
      "Train Accuracy for RandomForest: 0.7555\n",
      "Test Accuracy for RandomForest: 0.6692\n",
      "Train Precision for RandomForest: 0.8154\n",
      "Test Precision for RandomForest: 0.6964\n",
      "Train Recall for RandomForest: 0.6606\n",
      "Test Recall for RandomForest: 0.6000\n",
      "Computation Time for RandomForest: 164.78 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.4s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.08851759613930137), 'model__max_depth': 3, 'model__n_estimators': 373, 'model__subsample': np.float64(0.9628109945722505)}\n",
      "Train Accuracy for XGBClassifier: 0.8276\n",
      "Test Accuracy for XGBClassifier: 0.6882\n",
      "Train Precision for XGBClassifier: 0.8369\n",
      "Test Precision for XGBClassifier: 0.6843\n",
      "Train Recall for XGBClassifier: 0.8137\n",
      "Test Recall for XGBClassifier: 0.6986\n",
      "Computation Time for XGBClassifier: 70.68 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   8.2s\n",
      "Best model for RandomForest: {'model__max_depth': 7, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 2, 'model__min_samples_split': 8, 'model__n_estimators': 530}\n",
      "Train Accuracy for RandomForest: 0.7232\n",
      "Test Accuracy for RandomForest: 0.6597\n",
      "Train Precision for RandomForest: 0.7921\n",
      "Test Precision for RandomForest: 0.6917\n",
      "Train Recall for RandomForest: 0.6051\n",
      "Test Recall for RandomForest: 0.5763\n",
      "Computation Time for RandomForest: 170.13 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.2s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.06632882178455393), 'model__max_depth': 2, 'model__n_estimators': 274, 'model__subsample': np.float64(0.5143696269981928)}\n",
      "Train Accuracy for XGBClassifier: 0.7223\n",
      "Test Accuracy for XGBClassifier: 0.6777\n",
      "Train Precision for XGBClassifier: 0.7365\n",
      "Test Precision for XGBClassifier: 0.6784\n",
      "Train Recall for XGBClassifier: 0.6923\n",
      "Test Recall for XGBClassifier: 0.6758\n",
      "Computation Time for XGBClassifier: 70.09 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   7.4s\n",
      "Best model for RandomForest: {'model__max_depth': 7, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 2, 'model__min_samples_split': 8, 'model__n_estimators': 530}\n",
      "Train Accuracy for RandomForest: 0.7234\n",
      "Test Accuracy for RandomForest: 0.6681\n",
      "Train Precision for RandomForest: 0.7841\n",
      "Test Precision for RandomForest: 0.6975\n",
      "Train Recall for RandomForest: 0.6166\n",
      "Test Recall for RandomForest: 0.5936\n",
      "Computation Time for RandomForest: 154.20 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   1.9s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.0709996657782621), 'model__max_depth': 8, 'model__n_estimators': 440, 'model__subsample': np.float64(0.6560281881569949)}\n",
      "Train Accuracy for XGBClassifier: 1.0000\n",
      "Test Accuracy for XGBClassifier: 0.6793\n",
      "Train Precision for XGBClassifier: 1.0000\n",
      "Test Precision for XGBClassifier: 0.6761\n",
      "Train Recall for XGBClassifier: 1.0000\n",
      "Test Recall for XGBClassifier: 0.6883\n",
      "Computation Time for XGBClassifier: 69.70 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   1.3s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 15, 'model__n_estimators': 90}\n",
      "Train Accuracy for RandomForest: 0.7594\n",
      "Test Accuracy for RandomForest: 0.6745\n",
      "Train Precision for RandomForest: 0.8126\n",
      "Test Precision for RandomForest: 0.6908\n",
      "Train Recall for RandomForest: 0.6745\n",
      "Test Recall for RandomForest: 0.6318\n",
      "Computation Time for RandomForest: 133.03 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.2s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.06632882178455393), 'model__max_depth': 2, 'model__n_estimators': 274, 'model__subsample': np.float64(0.5143696269981928)}\n",
      "Train Accuracy for XGBClassifier: 0.7239\n",
      "Test Accuracy for XGBClassifier: 0.6751\n",
      "Train Precision for XGBClassifier: 0.7340\n",
      "Test Precision for XGBClassifier: 0.6722\n",
      "Train Recall for XGBClassifier: 0.7023\n",
      "Test Recall for XGBClassifier: 0.6835\n",
      "Computation Time for XGBClassifier: 64.68 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   1.2s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 15, 'model__n_estimators': 90}\n",
      "Train Accuracy for RandomForest: 0.7625\n",
      "Test Accuracy for RandomForest: 0.6612\n",
      "Train Precision for RandomForest: 0.8165\n",
      "Test Precision for RandomForest: 0.6784\n",
      "Train Recall for RandomForest: 0.6773\n",
      "Test Recall for RandomForest: 0.6129\n",
      "Computation Time for RandomForest: 121.74 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.6s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.04265407688058354), 'model__max_depth': 6, 'model__n_estimators': 224, 'model__subsample': np.float64(0.9687508340232414)}\n",
      "Train Accuracy for XGBClassifier: 0.9555\n",
      "Test Accuracy for XGBClassifier: 0.6671\n",
      "Train Precision for XGBClassifier: 0.9666\n",
      "Test Precision for XGBClassifier: 0.6548\n",
      "Train Recall for XGBClassifier: 0.9435\n",
      "Test Recall for XGBClassifier: 0.7067\n",
      "Computation Time for XGBClassifier: 62.00 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   1.6s\n",
      "Best model for RandomForest: {'model__max_depth': 7, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 14, 'model__min_samples_split': 10, 'model__n_estimators': 166}\n",
      "Train Accuracy for RandomForest: 0.7227\n",
      "Test Accuracy for RandomForest: 0.6664\n",
      "Train Precision for RandomForest: 0.7600\n",
      "Test Precision for RandomForest: 0.6778\n",
      "Train Recall for RandomForest: 0.6509\n",
      "Test Recall for RandomForest: 0.6343\n",
      "Computation Time for RandomForest: 114.35 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.3s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.014666566321361544), 'model__max_depth': 4, 'model__n_estimators': 280, 'model__subsample': np.float64(0.9104629857953324)}\n",
      "Train Accuracy for XGBClassifier: 0.7564\n",
      "Test Accuracy for XGBClassifier: 0.6754\n",
      "Train Precision for XGBClassifier: 0.7639\n",
      "Test Precision for XGBClassifier: 0.6686\n",
      "Train Recall for XGBClassifier: 0.7423\n",
      "Test Recall for XGBClassifier: 0.6955\n",
      "Computation Time for XGBClassifier: 60.02 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   2.3s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'log2', 'model__min_samples_leaf': 24, 'model__min_samples_split': 9, 'model__n_estimators': 347}\n",
      "Train Accuracy for RandomForest: 0.7372\n",
      "Test Accuracy for RandomForest: 0.6645\n",
      "Train Precision for RandomForest: 0.7722\n",
      "Test Precision for RandomForest: 0.6766\n",
      "Train Recall for RandomForest: 0.6731\n",
      "Test Recall for RandomForest: 0.6303\n",
      "Computation Time for RandomForest: 105.14 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.5s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.04265407688058354), 'model__max_depth': 6, 'model__n_estimators': 224, 'model__subsample': np.float64(0.9687508340232414)}\n",
      "Train Accuracy for XGBClassifier: 0.9724\n",
      "Test Accuracy for XGBClassifier: 0.6645\n",
      "Train Precision for XGBClassifier: 0.9793\n",
      "Test Precision for XGBClassifier: 0.6549\n",
      "Train Recall for XGBClassifier: 0.9653\n",
      "Test Recall for XGBClassifier: 0.6954\n",
      "Computation Time for XGBClassifier: 59.25 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   2.1s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'log2', 'model__min_samples_leaf': 24, 'model__min_samples_split': 9, 'model__n_estimators': 347}\n",
      "Train Accuracy for RandomForest: 0.7380\n",
      "Test Accuracy for RandomForest: 0.6731\n",
      "Train Precision for RandomForest: 0.7768\n",
      "Test Precision for RandomForest: 0.6892\n",
      "Train Recall for RandomForest: 0.6679\n",
      "Test Recall for RandomForest: 0.6307\n",
      "Computation Time for RandomForest: 98.43 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.4s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.02834045098534338), 'model__max_depth': 4, 'model__n_estimators': 323, 'model__subsample': np.float64(0.9722807884690141)}\n",
      "Train Accuracy for XGBClassifier: 0.8453\n",
      "Test Accuracy for XGBClassifier: 0.6590\n",
      "Train Precision for XGBClassifier: 0.8546\n",
      "Test Precision for XGBClassifier: 0.6475\n",
      "Train Recall for XGBClassifier: 0.8321\n",
      "Test Recall for XGBClassifier: 0.6979\n",
      "Computation Time for XGBClassifier: 55.83 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   4.3s\n",
      "Best model for RandomForest: {'model__max_depth': 7, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 2, 'model__min_samples_split': 8, 'model__n_estimators': 530}\n",
      "Train Accuracy for RandomForest: 0.7702\n",
      "Test Accuracy for RandomForest: 0.6673\n",
      "Train Precision for RandomForest: 0.8236\n",
      "Test Precision for RandomForest: 0.6797\n",
      "Train Recall for RandomForest: 0.6876\n",
      "Test Recall for RandomForest: 0.6329\n",
      "Computation Time for RandomForest: 92.31 seconds\n",
      "Fitting 25 folds for each of 20 candidates, totalling 500 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.4s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.02834045098534338), 'model__max_depth': 4, 'model__n_estimators': 323, 'model__subsample': np.float64(0.9722807884690141)}\n",
      "Train Accuracy for XGBClassifier: 0.8542\n",
      "Test Accuracy for XGBClassifier: 0.6673\n",
      "Train Precision for XGBClassifier: 0.8615\n",
      "Test Precision for XGBClassifier: 0.6582\n",
      "Train Recall for XGBClassifier: 0.8440\n",
      "Test Recall for XGBClassifier: 0.6960\n",
      "Computation Time for XGBClassifier: 54.28 seconds\n"
     ]
    }
   ],
   "source": [
    "df_rf_feature_selection = pd.read_csv(project_path()+r\"/data/target_data/rf_feature_selection.csv\", index_col=0)\n",
    "df_ml = pd.read_csv(project_path()+r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "\n",
    "results = {}\n",
    "\n",
    "hours = [i for i in range(1, 13)]\n",
    "for hour in hours:\n",
    "    df_ml_hour = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=hour, hours_before_sepsis_cutoff=hour-1)\n",
    "\n",
    "    df_ml_train = df_ml_hour.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\") \n",
    "    df_ml_test = df_ml_hour.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\")\n",
    "\n",
    "    X_train = df_ml_train.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "    y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "    X_test = df_ml_test.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "    y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "    pipelines = {\n",
    "        'RandomForest': Pipeline([\n",
    "            ('model', RandomForestClassifier(random_state=42))\n",
    "        ], verbose=True),\n",
    "\n",
    "        'XGBClassifier': Pipeline([\n",
    "            ('model', XGBClassifier(random_state=42))\n",
    "        ], verbose=True),\n",
    "    }\n",
    "\n",
    "    param_grids = {\n",
    "        'RandomForest': {\n",
    "            'model__n_estimators': randint(10, 600),\n",
    "            'model__max_depth': randint(1, 9),\n",
    "            'model__min_samples_split': randint(2, 25),\n",
    "            'model__min_samples_leaf': randint(2, 25),\n",
    "            'model__max_features': ['sqrt', 'log2'],\n",
    "        },\n",
    "        'XGBClassifier': {\n",
    "            'model__n_estimators': randint(10, 450),\n",
    "            'model__max_depth': randint(1, 9),\n",
    "            'model__learning_rate': uniform(0.01, 0.1),\n",
    "            'model__subsample': uniform(0.5, 0.9),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    gkf = GroupKFold(n_splits=25)\n",
    "    groups = df_ml_train[\"ICUSTAY_ID\"]\n",
    "\n",
    "    # Perform RandomizedSearchCV for each pipeline\n",
    "    for name, pipeline in pipelines.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        search = RandomizedSearchCV(pipeline, param_distributions=param_grids[name], \n",
    "                                    n_iter=20, cv=gkf, random_state=42, n_jobs=-1, verbose=3)\n",
    "        search.fit(X_train, y_train, groups=groups)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Get the best model\n",
    "        best_model = search.best_estimator_\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        train_precision = precision_score(y_train, y_train_pred)\n",
    "        test_precision = precision_score(y_test, y_test_pred)\n",
    "        train_recall = recall_score(y_train, y_train_pred)\n",
    "        test_recall = recall_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Save the best model\n",
    "        model_filename = project_path()+fr\"/machine_learning/models/{name}_presel_hour_{hour}.joblib\"\n",
    "        joblib.dump(best_model, model_filename)\n",
    "        \n",
    "        print(f\"Best model for {name}: {search.best_params_}\")\n",
    "        print(f\"Train Accuracy for {name}: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Accuracy for {name}: {test_accuracy:.4f}\")\n",
    "        print(f\"Train Precision for {name}: {train_precision:.4f}\")\n",
    "        print(f\"Test Precision for {name}: {test_precision:.4f}\")\n",
    "        print(f\"Train Recall for {name}: {train_recall:.4f}\")\n",
    "        print(f\"Test Recall for {name}: {test_recall:.4f}\")\n",
    "        print(f\"Computation Time for {name}: {results[name]['computation_time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost - Hours stacked clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_feature_selection = pd.read_csv(project_path()+r\"/data/target_data/rf_feature_selection.csv\", index_col=0)\n",
    "df_ml = pd.read_csv(project_path()+r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_ml = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=12, hours_before_sepsis_cutoff=0)\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\") \n",
    "df_ml_test = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\")\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test = df_ml_test.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "xgb_hour_1 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_1.joblib\")\n",
    "xgb_hour_2 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_2.joblib\")\n",
    "xgb_hour_3 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_3.joblib\")\n",
    "xgb_hour_4 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_4.joblib\")\n",
    "xgb_hour_5 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_5.joblib\")\n",
    "xgb_hour_6 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_6.joblib\")\n",
    "xgb_hour_7 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_7.joblib\")\n",
    "xgb_hour_8 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_8.joblib\")\n",
    "xgb_hour_9 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_9.joblib\")\n",
    "xgb_hour_10 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_10.joblib\")\n",
    "xgb_hour_11 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_11.joblib\")\n",
    "xgb_hour_12 = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_presel_hour_12.joblib\")\n",
    "\n",
    "base_clf = [\n",
    "    (\"xgb_hour_1\", xgb_hour_1),\n",
    "    (\"xgb_hour_2\", xgb_hour_2),\n",
    "    (\"xgb_hour_3\", xgb_hour_3),\n",
    "    (\"xgb_hour_4\", xgb_hour_4),\n",
    "    (\"xgb_hour_5\", xgb_hour_5),\n",
    "    (\"xgb_hour_6\", xgb_hour_6),\n",
    "    (\"xgb_hour_7\", xgb_hour_7),\n",
    "    (\"xgb_hour_8\", xgb_hour_8),\n",
    "    (\"xgb_hour_9\", xgb_hour_9),\n",
    "    (\"xgb_hour_10\", xgb_hour_10),\n",
    "    (\"xgb_hour_11\", xgb_hour_11),\n",
    "    (\"xgb_hour_12\", xgb_hour_12)\n",
    "]\n",
    "\n",
    "meta_model  = LogisticRegression(random_state=42)\n",
    "stack       = StackingClassifier(estimators=base_clf, final_estimator=meta_model, cv=20, n_jobs=-1, verbose=3)\n",
    "start_time  = time.time()\n",
    "stack.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "model_filename = project_path()+fr\"/machine_learning/models/StackedXGB.joblib\"\n",
    "joblib.dump(stack, model_filename)\n",
    "\n",
    "# Make predictions with the stack\n",
    "y_train_pred = stack.predict(X_train)\n",
    "y_test_pred = stack.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy  = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy   = accuracy_score(y_test, y_test_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "test_precision  = precision_score(y_test, y_test_pred)\n",
    "train_recall    = recall_score(y_train, y_train_pred)\n",
    "test_recall     = recall_score(y_test, y_test_pred)\n",
    "\n",
    "results = {}\n",
    "\n",
    "results['StackedXGB'] = {\n",
    "    'model': 'StackedXGB',\n",
    "    'pipeline': 'StackedXGB',\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'train_precision': train_precision,\n",
    "    'test_precision': test_precision,\n",
    "    'train_recall': train_recall,\n",
    "    'test_recall': test_recall,\n",
    "    'best_params': stack.get_params(),\n",
    "    'computation_time': end_time - start_time,\n",
    "    'model_path': model_filename,\n",
    "    'feature_importances': None,\n",
    "    'selected_features': stack.feature_names_in_.tolist(),\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path()+fr\"/machine_learning/results/ml_results_StackedXGB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Hours stacked clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_feature_selection = pd.read_csv(project_path()+r\"/data/target_data/rf_feature_selection.csv\", index_col=0)\n",
    "df_ml = pd.read_csv(project_path()+r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_ml = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=12, hours_before_sepsis_cutoff=0)\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\") \n",
    "df_ml_test = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\")\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test = df_ml_test.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "\n",
    "RF_hour_1 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_1.joblib\")\n",
    "RF_hour_2 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_2.joblib\")\n",
    "RF_hour_3 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_3.joblib\")\n",
    "RF_hour_4 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_4.joblib\")\n",
    "RF_hour_5 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_5.joblib\")\n",
    "RF_hour_6 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_6.joblib\")\n",
    "RF_hour_7 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_7.joblib\")\n",
    "RF_hour_8 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_8.joblib\")\n",
    "RF_hour_9 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_9.joblib\")\n",
    "RF_hour_10 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_10.joblib\")\n",
    "RF_hour_11 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_11.joblib\")\n",
    "RF_hour_12 = joblib.load(project_path()+r\"/machine_learning/models/RandomForest_presel_hour_12.joblib\")\n",
    "\n",
    "base_clf = [\n",
    "    (\"RF_hour_1\", RF_hour_1),\n",
    "    (\"RF_hour_2\", RF_hour_2),\n",
    "    (\"RF_hour_3\", RF_hour_3),\n",
    "    (\"RF_hour_4\", RF_hour_4),\n",
    "    (\"RF_hour_5\", RF_hour_5),\n",
    "    (\"RF_hour_6\", RF_hour_6),\n",
    "    (\"RF_hour_7\", RF_hour_7),\n",
    "    (\"RF_hour_8\", RF_hour_8),\n",
    "    (\"RF_hour_9\", RF_hour_9),\n",
    "    (\"RF_hour_10\", RF_hour_10),\n",
    "    (\"RF_hour_11\", RF_hour_11),\n",
    "    (\"RF_hour_12\", RF_hour_12)\n",
    "]\n",
    "\n",
    "meta_model  = LogisticRegression(random_state=42)\n",
    "stack       = StackingClassifier(estimators=base_clf, final_estimator=meta_model, cv=20, n_jobs=-1, verbose=3)\n",
    "start_time  = time.time()\n",
    "stack.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "model_filename = project_path()+fr\"/machine_learning/models/StackedRF.joblib\"\n",
    "joblib.dump(stack, model_filename)\n",
    "\n",
    "# Make predictions with the stack\n",
    "y_train_pred = stack.predict(X_train)\n",
    "y_test_pred = stack.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy  = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy   = accuracy_score(y_test, y_test_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "test_precision  = precision_score(y_test, y_test_pred)\n",
    "train_recall    = recall_score(y_train, y_train_pred)\n",
    "test_recall     = recall_score(y_test, y_test_pred)\n",
    "\n",
    "results = {}\n",
    "\n",
    "results['StackedRF'] = {\n",
    "    'model': 'StackedRF',\n",
    "    'pipeline': 'StackedRF',\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'train_precision': train_precision,\n",
    "    'test_precision': test_precision,\n",
    "    'train_recall': train_recall,\n",
    "    'test_recall': test_recall,\n",
    "    'best_params': stack.get_params(),\n",
    "    'computation_time': end_time - start_time,\n",
    "    'model_path': model_filename,\n",
    "    'feature_importances': None,\n",
    "    'selected_features': stack.feature_names_in_.tolist(),\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path()+fr\"/machine_learning/results/ml_results_StackedRF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost and Logistic Regreesion - Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = pd.read_csv(project_path()+r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_ml = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=12, hours_before_sepsis_cutoff=0)\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\") \n",
    "df_ml_test = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\")\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test = df_ml_test.iloc[:, 10:]\n",
    "y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "\n",
    "xgb = joblib.load(project_path()+r\"/machine_learning/models/XGBClassifier_all.joblib\")\n",
    "lr = joblib.load(project_path()+r\"/machine_learning/models/LogisticRegression_lasso_alpha_search.joblib\")\n",
    "\n",
    "\n",
    "base_clf = [\n",
    "    (\"xgb\", xgb),\n",
    "    (\"lr\", lr),\n",
    "]\n",
    "\n",
    "meta_model  = LogisticRegression(random_state=42)\n",
    "stack       = StackingClassifier(estimators=base_clf, final_estimator=meta_model, cv=20, n_jobs=-1, verbose=3)\n",
    "start_time  = time.time()\n",
    "stack.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "model_filename = project_path()+fr\"/machine_learning/models/StackedXGB_LR.joblib\"\n",
    "joblib.dump(stack, model_filename)\n",
    "\n",
    "# Make predictions with the stack\n",
    "y_train_pred = stack.predict(X_train)\n",
    "y_test_pred = stack.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy  = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy   = accuracy_score(y_test, y_test_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "test_precision  = precision_score(y_test, y_test_pred)\n",
    "train_recall    = recall_score(y_train, y_train_pred)\n",
    "test_recall     = recall_score(y_test, y_test_pred)\n",
    "\n",
    "results = {}\n",
    "\n",
    "results['StackedXGB_LR'] = {\n",
    "    'model': 'StackedXGB_LR',\n",
    "    'pipeline': 'StackedXGB_LR',\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'train_precision': train_precision,\n",
    "    'test_precision': test_precision,\n",
    "    'train_recall': train_recall,\n",
    "    'test_recall': test_recall,\n",
    "    'best_params': stack.get_params(),\n",
    "    'computation_time': end_time - start_time,\n",
    "    'model_path': model_filename,\n",
    "    'feature_importances': None,\n",
    "    'selected_features': stack.feature_names_in_.tolist(),\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path()+fr\"/machine_learning/results/ml_results_StackedXGB_LR.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack stacked hours Random Forest and stacked hours XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_feature_selection = pd.read_csv(project_path()+r\"/data/target_data/rf_feature_selection.csv\", index_col=0)\n",
    "df_ml = pd.read_csv(project_path()+r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_ml = ml_functions.make_df_for_ml(df_ml, hours_before_sepsis_onset=12, hours_before_sepsis_cutoff=0)\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\") \n",
    "df_ml_test = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\")\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test = df_ml_test.iloc[:, 10:].loc[:, df_rf_feature_selection[\"Feature\"]]\n",
    "y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "rf = joblib.load(project_path()+r\"/machine_learning/models/StackedRF.joblib\")\n",
    "xgb = joblib.load(project_path()+r\"/machine_learning/models/StackedXGB.joblib\")\n",
    "\n",
    "base_clf = [\n",
    "    (\"rf\", rf),\n",
    "    (\"xgb\", xgb),\n",
    "]\n",
    "\n",
    "meta_model  = LogisticRegression(random_state=42)\n",
    "stack       = StackingClassifier(estimators=base_clf, final_estimator=meta_model, cv=20, n_jobs=-1, verbose=3)\n",
    "start_time  = time.time()\n",
    "stack.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "model_filename = project_path()+fr\"/machine_learning/models/StackedRF_XGB.joblib\"\n",
    "joblib.dump(stack, model_filename)\n",
    "\n",
    "# Make predictions with the stack\n",
    "y_train_pred = stack.predict(X_train)\n",
    "y_test_pred = stack.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy  = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy   = accuracy_score(y_test, y_test_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "test_precision  = precision_score(y_test, y_test_pred)\n",
    "train_recall    = recall_score(y_train, y_train_pred)\n",
    "test_recall     = recall_score(y_test, y_test_pred)\n",
    "\n",
    "results = {}\n",
    "\n",
    "results['StackedRF_XGB'] = {\n",
    "    'model': 'StackedRF_XGB',\n",
    "    'pipeline': 'StackedRF_XGB',\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'train_precision': train_precision,\n",
    "    'test_precision': test_precision,\n",
    "    'train_recall': train_recall,\n",
    "    'test_recall': test_recall,\n",
    "    'best_params': stack.get_params(),\n",
    "    'computation_time': end_time - start_time,\n",
    "    'model_path': model_filename,\n",
    "    'feature_importances': None,\n",
    "    'selected_features': stack.feature_names_in_.tolist(),\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path()+fr\"/machine_learning/results/ml_results_StackedRF_XGB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest and XGBoost - Reference first hour no engineered features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   2.2s\n",
      "Best model for RandomForest: {'model__max_depth': 8, 'model__max_features': 'log2', 'model__min_samples_leaf': 23, 'model__min_samples_split': 9, 'model__n_estimators': 347}\n",
      "Train Accuracy for RandomForest: 0.7210\n",
      "Test Accuracy for RandomForest: 0.6564\n",
      "Train Precision for RandomForest: 0.7628\n",
      "Test Precision for RandomForest: 0.6782\n",
      "Train Recall for RandomForest: 0.6416\n",
      "Test Recall for RandomForest: 0.5953\n",
      "Computation Time for RandomForest: 58.41 seconds\n",
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=   0.2s\n",
      "Best model for XGBClassifier: {'model__learning_rate': np.float64(0.02834045098534338), 'model__max_depth': 4, 'model__n_estimators': 323, 'model__subsample': np.float64(0.9722807884690141)}\n",
      "Train Accuracy for XGBClassifier: 0.7668\n",
      "Test Accuracy for XGBClassifier: 0.6588\n",
      "Train Precision for XGBClassifier: 0.7914\n",
      "Test Precision for XGBClassifier: 0.6641\n",
      "Train Recall for XGBClassifier: 0.7246\n",
      "Test Recall for XGBClassifier: 0.6427\n",
      "Computation Time for XGBClassifier: 12.96 seconds\n"
     ]
    }
   ],
   "source": [
    "df_ml = pd.read_csv(project_path()+r\"/data/target_data/df_ml.csv\", index_col=0)\n",
    "df_ml = df_ml.groupby(\"ICUSTAY_ID\").first().reset_index().iloc[:,:47]\n",
    "\n",
    "df_ml_train = df_ml.merge(df_train_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\") \n",
    "df_ml_test = df_ml.merge(df_test_ids, on=[\"SEPSIS_ID\", \"RANDOM_NO_SEPSIS_ID\"], how=\"inner\")\n",
    "\n",
    "X_train = df_ml_train.iloc[:, 10:]\n",
    "y_train = df_ml_train[\"SEPSIS_LABEL\"]\n",
    "X_test = df_ml_test.iloc[:, 10:]\n",
    "y_test = df_ml_test[\"SEPSIS_LABEL\"]\n",
    "\n",
    "pipelines = {\n",
    "    'RandomForest': Pipeline([\n",
    "        ('model', RandomForestClassifier(random_state=42))\n",
    "    ], verbose=True),\n",
    "\n",
    "    'XGBClassifier': Pipeline([\n",
    "        ('model', XGBClassifier(random_state=42))\n",
    "    ], verbose=True),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': randint(10, 600),\n",
    "        'model__max_depth': randint(1, 9),\n",
    "        'model__min_samples_split': randint(2, 25),\n",
    "        'model__min_samples_leaf': randint(1, 25),\n",
    "        'model__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model__n_estimators': randint(10, 600),\n",
    "        'model__max_depth': randint(1, 9),\n",
    "        'model__learning_rate': uniform(0.01, 0.1),\n",
    "        'model__subsample': uniform(0.5, 0.9),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Perform RandomizedSearchCV for each pipeline\n",
    "for name, pipeline in pipelines.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    search = RandomizedSearchCV(pipeline, param_distributions=param_grids[name], \n",
    "                                n_iter=20, cv=20, random_state=42, n_jobs=-1, verbose=3)\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    best_model = search.best_estimator_\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    \n",
    "    model_filename = project_path()+fr\"/machine_learning/models/{name}_first_hour.joblib\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    \n",
    "    # Store feature importances directly in the results dictionary\n",
    "    feature_importances = None\n",
    "    if hasattr(best_model.named_steps['model'], 'feature_importances_'):\n",
    "        feature_importances = pd.Series(\n",
    "            best_model.named_steps['model'].feature_importances_,\n",
    "            index=X_train.columns\n",
    "        ).to_dict() \n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[name] = {\n",
    "        'model': name,\n",
    "        'pipeline': 'first_hour',\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_precision': train_precision,\n",
    "        'test_precision': test_precision,\n",
    "        'train_recall': train_recall,\n",
    "        'test_recall': test_recall,\n",
    "        'best_params': search.best_params_,\n",
    "        'computation_time': end_time - start_time,\n",
    "        'model_path': model_filename,\n",
    "        'feature_importances': feature_importances,\n",
    "        'selected_features': X_train.columns.tolist()\n",
    "    }\n",
    "\n",
    "    print(f\"Best model for {name}: {search.best_params_}\")\n",
    "    print(f\"Train Accuracy for {name}: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy for {name}: {test_accuracy:.4f}\")\n",
    "    print(f\"Train Precision for {name}: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision for {name}: {test_precision:.4f}\")\n",
    "    print(f\"Train Recall for {name}: {train_recall:.4f}\")\n",
    "    print(f\"Test Recall for {name}: {test_recall:.4f}\")\n",
    "    print(f\"Computation Time for {name}: {results[name]['computation_time']:.2f} seconds\")\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_results.to_csv(project_path()+fr\"/machine_learning/results/ml_results__first_hour.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
